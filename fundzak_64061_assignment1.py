# -*- coding: utf-8 -*-
"""Fundzak_64061_Assignment1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1agD4lkD1V9NW2mDumtSQSh6cthZux45f

### The IMDB dataset

Assignment 1 Advanced Machine Learning

**Loading the IMDB dataset**
"""

from tensorflow.keras.datasets import imdb
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(
    num_words=10000)

train_data[0]

train_labels[0]

max([max(sequence) for sequence in train_data])

"""**Decoding reviews back to text**"""

word_index = imdb.get_word_index()
reverse_word_index = dict(
    [(value, key) for (key, value) in word_index.items()])
decoded_review = " ".join(
    [reverse_word_index.get(i - 3, "?") for i in train_data[0]])

"""### Preparing the data

**Encoding the integer sequences via multi-hot encoding**
"""

import numpy as np
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
    return results
x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

x_train[0]

y_train = np.asarray(train_labels).astype("float32")
y_test = np.asarray(test_labels).astype("float32")

"""### Building your model

**Model definition**
"""

from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

"""**Compiling the model**"""

model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

"""### Validating your approach

**Setting aside a validation set**
"""

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

"""**Training your model**"""

history = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict = history.history
history_dict.keys()

"""**Plotting the training and validation loss**"""

import matplotlib.pyplot as plt
history_dict = history.history
loss_values = history_dict["loss"]
val_loss_values = history_dict["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

"""**Plotting the training and validation accuracy**"""

plt.clf()
acc = history_dict["accuracy"]
val_acc = history_dict["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""**Retraining a model from scratch**"""

model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.fit(x_train, y_train, epochs=4, batch_size=512)
results = model.evaluate(x_test, y_test)

results



"""**USING 3 HIDDEN LAYERS TO SEE HOW VALIDATION AND TEST ACCURACY ARE AFFECTED**"""

#model with 3 hidden layers instead of 2
from tensorflow import keras
from tensorflow.keras import layers

three_layer_model = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

three_layer_model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history_three_layer_model = model.fit(partial_x_train,
                    partial_y_train,
                    epochs=10,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict_3layers = history.history
history_dict_3layers.keys()

import matplotlib.pyplot as plt
history_dict = history_three_layer_model.history
loss_values = history_dict_3layers["loss"]
val_loss_values = history_dict_3layers["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = history_dict_3layers["accuracy"]
val_acc = history_dict_3layers["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""QUESTION 1: By adding a third hidden layer to the data set,the accuracy becomes more unsteady than having two layers. There is a continous decrease in the validationn accuracy in both 2 hidden and 3 hidden layers but 3 hidden layers experiences more sudden drops in accuracy of the vallidation. Training seems to inncrease at a slightly higher rate in the 3 hidden layer experientment showing the two hidden layers was overall a better fit for the dataset.

**USING THE ORIGINAL 2 LAYERS WITH MORE HIDDEN UNITS. AND INCREASE FROM 16 TO 32**
"""

#model with 2 hidden layers and a chnage to 32 units each within the hidden layers
from tensorflow import keras
from tensorflow.keras import layers

hiddenunits32 = keras.Sequential([
    layers.Dense(32, activation="relu"),
    layers.Dense(32, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

hiddenunits32.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history_hiddenunits32 = hiddenunits32.fit(partial_x_train,
                    partial_y_train,
                    epochs=10,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict_hiddenunits32 = history.history
history_dict_hiddenunits32.keys()

import matplotlib.pyplot as plt
history_dict_hiddenunits32 = history_hiddenunits32.history
loss_values = history_dict_hiddenunits32["loss"]
val_loss_values = history_dict_hiddenunits32["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = history_dict_hiddenunits32["accuracy"]
val_acc = history_dict_hiddenunits32["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""QUESTION 2: Adding more units to the hidden layer seems to increase the accuracy of the validation data. The accuracy tends to hover around .875 which is a higher average than what it holds at 16 units.

USING THE MSE LOSS FUNCTION IN PLACE OF THE BINARY_CROSSENTROPY
"""

#Reverting back to the original model of 2 hidden layers, 16 units in order to compare the MSE Loss function to the binary_crossentrapy
from tensorflow import keras
from tensorflow.keras import layers

MSEmodel = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

"""Here is where you would insert the new loss function, MSE"""

MSEmodel.compile(optimizer="rmsprop",
              loss="MSE",
              metrics=["accuracy"])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history_MSEmodel = MSEmodel.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict_MSEmodel = history.history
history_dict_MSEmodel.keys()

import matplotlib.pyplot as plt
history_dict_MSEmodel = history_MSEmodel.history
loss_values = history_dict_MSEmodel["loss"]
val_loss_values = history_dict_MSEmodel["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = history_dict_MSEmodel["accuracy"]
val_acc = history_dict_MSEmodel["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""QUESTION 3: Using MSE loss function made the validation accuracy stay more consistent with reduced peaks and bottom out. The MSE function seems to work slightly better for this problem.

USINNG TANH ACTIVATION IN PLACE OF RELU ACTIVATION FUNNCTION
"""

#Right off the bat is where we introduce the tanh activationn function
from tensorflow import keras
from tensorflow.keras import layers

TANHmodel = keras.Sequential([
    layers.Dense(16, activation="tanh"),
    layers.Dense(16, activation="tanh"),
    layers.Dense(1, activation="sigmoid")
])

#Here I go back to binary_crossentropy to realte results back to original functions
TANHmodel.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history_TANHmodel = TANHmodel.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict_TANHmodel = history.history
history_dict_TANHmodel.keys()

import matplotlib.pyplot as plt
history_dict_TANHmodel = history_TANHmodel.history
loss_values = history_dict_TANHmodel["loss"]
val_loss_values = history_dict_TANHmodel["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = history_dict_TANHmodel["accuracy"]
val_acc = history_dict_TANHmodel["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""QUESTION 4: Tanh activation shows a pretty dramatic increase in fall of accuracy that comes with tanh activiation as supposed to relu for this data set. There are more inconsistencies and spikes in the decrease in the validation accuary of the data with tanh activation

**Perfroming Dropout on the model for better validation. Dropout was expressed as a leading function in the field to help with validation so that is why I chose to go with this function.**
"""

#Dropout is implemented in this part of the code. I use dropout within the first hidden layer. I choose a dropout of .8 in hopes of optimal and max accuracy at the end..
from tensorflow import keras
from tensorflow.keras import layers

DROPOUTmodel = keras.Sequential([
    layers.Dense(16, activation="relu"),
    layers.Dropout(.8),
    layers.Dense(16, activation="relu"),
    layers.Dense(1, activation="sigmoid")
])

DROPOUTmodel.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])

x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]

history_DROPOUTmodel = DROPOUTmodel.fit(partial_x_train,
                    partial_y_train,
                    epochs=20,
                    batch_size=512,
                    validation_data=(x_val, y_val))

history_dict_DROPOUTmodel = history.history
history_dict_DROPOUTmodel.keys()

import matplotlib.pyplot as plt
history_dict_DROPOUTmodel = history_DROPOUTmodel.history
loss_values = history_dict_DROPOUTmodel["loss"]
val_loss_values = history_dict_DROPOUTmodel["val_loss"]
epochs = range(1, len(loss_values) + 1)
plt.plot(epochs, loss_values, "bo", label="Training loss")
plt.plot(epochs, val_loss_values, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.clf()
acc = history_dict_DROPOUTmodel["accuracy"]
val_acc = history_dict_DROPOUTmodel["val_accuracy"]
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

"""QUESTION 5: Using dropout quickly clims the validation accuracy to reach that of the training accuracy whihc is very good but then we start to see some overfitting so some adjustments would need to be made in a real world scenario to find a longer standing accuracy fit without going to overfitting. A really good start ot that meawsure though. Maybe reducing the dropout rate or adding a hidden layer could draw different results. """
